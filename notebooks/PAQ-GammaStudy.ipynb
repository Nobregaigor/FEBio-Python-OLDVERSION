{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "4ce963eec525c72576d8dfcde812f8487bbeefd0db94f66638320c10fe71db33"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Info\n",
    "This notebook is used to read, combine and save the results from a series of simulations\n",
    "\n",
    "- The main inputs are the feb filename and the directory where all simulations where run (they can be stored in sub-directories)\n",
    "- The main output is a dataframe with the combined dataset from all simulations\n",
    "\n",
    "- Additional inputs that can be left as default are the \n",
    "\n",
    " -- Data keys (for nodes and elements data txt files)\n",
    "\n",
    " -- Log filename (csv file containg the run and parameter information)\n",
    "\n",
    " -- Dataframe types (how each datapoint sould be interpreted and saved)\n",
    " \n",
    " -- Read from raw txt or extract data from pickles\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "To use our modules from FEBio-Python, this notebook must be run inside a \"notebooks\" folder that shared the \"src\" directory of FEBio-Python or we must append the path to the src directory"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "from os import path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append('../src/')\n",
    "\n",
    "from modules.sys_functions.find_files_in_folder import find_files, find_folders, search_dirs\n",
    "from modules.classes import Data_Manipulator"
   ]
  },
  {
   "source": [
    "# Set paths and load files\n",
    "\n",
    "Here we are going to set the main inputs to run this notebook.\n",
    "\n",
    "-- runs_dir: path to directory containg simulations (each simulation should be in a sub-directory named with its run number ->  run-%s  <- )\n",
    "\n",
    "-- feb_filename: name of the feb_file to look for. It will not work if this file is not with its correct name"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Inputs\n",
    "runs_dir = \"D:\\\\Igor\\\\Research_USF\\\\University of South Florida\\\\Mao, Wenbin - Igor\\\\Febio-Models\\\\Active-Models\\\\PAQ\\\\Gamma-0-05\\\\runs\"\n",
    "feb_filename = \"myo_hex_coarse_6_epi_60_endo_-60.feb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Optional Inputs\"\n",
    "elems_keys = \"sx;sy;sz;sxy;sxz;syz\"\n",
    "nodes_keys = \"x;y;z;ux;uy;uz\"\n",
    "nodes_filename = \"node.txt\"\n",
    "elems_filename = \"elem.txt\"\n",
    "log_filename = \"run_log.csv\"\n",
    "\n",
    "df_dtypes = {\n",
    "    'x': 'float32',\n",
    "    'y': 'float32',\n",
    "    'z': 'float32',\n",
    "    'ux': 'float32',\n",
    "    'uy': 'float32',\n",
    "    'uz': 'float32',\n",
    "    'node': 'category',\n",
    "    'timestep': 'float32',\n",
    "    'run_ref': 'category', #'uint8',\n",
    "    'param_val': 'category', #'float32',\n",
    "    'sx': 'float32',\n",
    "    'sy': 'float32',\n",
    "    'sz': 'float32',\n",
    "    'sxy': 'float32',\n",
    "    'sxz': 'float32',\n",
    "    'syz': 'float32',\n",
    "    'elem': 'category'\n",
    "    }\n",
    "\n",
    "# This bool will check if the program should read raw txt files directly from results or read pre-created pickle files\n",
    "READ_PLOT_FILES = True\n",
    "USE_NUM_RUN_LOG = True\n",
    "\n",
    "# This bool lets you use this notebook for a single file\n",
    "SINGLE_FILE = False\n",
    "PARAM_VAL = 1 # Only used if SINGLE_FILE is true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths and find files\n",
    "\n",
    "log_filepath = path.join(runs_dir, log_filename)\n",
    "run_dirs = find_folders(runs_dir)\n",
    "runs = []\n",
    "runs_nums = []\n",
    "for (dp, dd, dn) in run_dirs:\n",
    "    if dn != \"pickles\":\n",
    "        _files = []\n",
    "        feb_files = find_files(dp, (\"fileFormat\", \"feb\"))\n",
    "        txt_files = find_files(dp, (\"fileFormat\", \"txt\"))\n",
    "        _files.extend(feb_files)\n",
    "        _files.extend(txt_files)\n",
    "        runs.append(_files)\n",
    "        if not SINGLE_FILE:\n",
    "            if USE_NUM_RUN_LOG:\n",
    "                runs_nums.append(int(dn.split(\"-\")[-1]))\n",
    "            else:\n",
    "                runs_nums.append(dn)\n",
    "        else:\n",
    "            runs_nums.append(1)\n",
    "\n",
    "if not SINGLE_FILE:\n",
    "    log_df = pd.read_csv(log_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    run#  node#  param\n",
       "0      0     11  0.000\n",
       "1      1     12  0.005\n",
       "2      2     13  0.010\n",
       "3      3     14  0.015\n",
       "4      4      0  0.020\n",
       "5      5      1  0.025\n",
       "6      6      2  0.030\n",
       "7      7      3  0.035\n",
       "8      8      4  0.040\n",
       "9      9      5  0.045\n",
       "10    10      6  0.050"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>run#</th>\n      <th>node#</th>\n      <th>param</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>11</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>12</td>\n      <td>0.005</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>13</td>\n      <td>0.010</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>14</td>\n      <td>0.015</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0</td>\n      <td>0.020</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>1</td>\n      <td>0.025</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>2</td>\n      <td>0.030</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>3</td>\n      <td>0.035</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8</td>\n      <td>4</td>\n      <td>0.040</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9</td>\n      <td>5</td>\n      <td>0.045</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>10</td>\n      <td>6</td>\n      <td>0.050</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "log_df"
   ]
  },
  {
   "source": [
    "# Create DataManipulators\n",
    "\n",
    "DataManipulator are useful to extract nodes and elements data and combina all information within a single dataframe. Remember that nodes and elements contains different data, with different lengths, and, becuase of this, they cannot be analyzed directly. Therefore, DataManipulator will compute all nodal data from elements, and, in the end, we will have all information based on nodal coordinates. \n",
    "\n",
    "Since we have the same geometry, we can use one DataManipulator as a reference. The dataframe reads the feb file and save as reference a nodes and elements dictionaries, which contains the correlation between nodes and elements in the mesh. If we want to check different feb files, we must have different datamanipulators."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This functions checks the total memory usage os a dataframe\n",
    "def mem_usage(pandas_obj):\n",
    "    if isinstance(pandas_obj,pd.DataFrame):\n",
    "        usage_b = pandas_obj.memory_usage(deep=True).sum()\n",
    "    else: # we assume if not a df it's a series\n",
    "        usage_b = pandas_obj.memory_usage(deep=True)\n",
    "    usage_mb = usage_b / 1024 ** 2 # convert bytes to megabytes\n",
    "    return usage_mb"
   ]
  },
  {
   "source": [
    "### Read raw data\n",
    "\n",
    "The next node reads and saves raw nodes and elements files\n",
    "* to save memory usage, we will be reading data from nodes/elements and saving into a pickle file and reading each file later"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "'x'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32md:\\Igor\\GitHub\\FEBio-Python\\venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2645\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2646\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2647\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'x'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-c07aca636ada>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mnodes_file\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0melems_file\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m                 \u001b[0mpickle_filename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"data-run-{v}.pickle\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m                 \u001b[0mnew_dm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_plot_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnodes_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melems_file\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnodes_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melems_keys\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"node\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"elem\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_dtypes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m                 \u001b[0mm_use\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mmem_usage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_dm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m                 \u001b[0mf_ctn\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Igor\\GitHub\\FEBio-Python\\src\\modules\\classes\\Data_Manipulator.py\u001b[0m in \u001b[0;36mread_plot_file\u001b[1;34m(self, files, data_formats, refs, out_file_name, run_ref, param_val, dtypes, save_pickle)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcolumn_key\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m             \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolumn_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn_key\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn_key\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Igor\\GitHub\\FEBio-Python\\venv\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2798\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2799\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2800\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2801\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2802\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Igor\\GitHub\\FEBio-Python\\venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2646\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2647\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2648\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2650\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'x'"
     ]
    }
   ],
   "source": [
    "pickles_dir = path.join(runs_dir, \"pickles\")\n",
    "tpm_pickles_dir = path.join(pickles_dir, \"tpm\")\n",
    "\n",
    "feb_file_ref = runs[0][0][0] # Using same file as ref since geometry is the same <-- MUST CHANGE IF WE USE DIFFERENT GEOMETRY\n",
    "new_dm = Data_Manipulator(feb_file_ref) # Using same file as ref since geometry is the same <-- MUST CHANGE IF WE USE DIFFERENT GEOMETRY\n",
    "\n",
    "m_use = 0\n",
    "f_ctn = 0\n",
    "\n",
    "# if LOAD_FROM_PICKLES == True:\n",
    "#     dms = []\n",
    "    \n",
    "#     # print(feb_file_ref)\n",
    "#     pickles_paths = search_dirs(tpm_pickles_dir, \".pickle\")\n",
    "#     for pp in pickles_paths:\n",
    "#         # new_dm = Data_Manipulator(feb_file_ref)\n",
    "#         new_dm.set_data(pickle_path=pp)\n",
    "#         dms.append(new_dm)\n",
    "\n",
    "if READ_PLOT_FILES:\n",
    "    if not path.exists(tpm_pickles_dir):\n",
    "        os.makedirs(tpm_pickles_dir) \n",
    "\n",
    "    # dms = []\n",
    "    for i, run in enumerate(runs):\n",
    "        feb_file = nodes_file = elems_file = None\n",
    "        for (fp, ff, fn) in run:\n",
    "            if ff == feb_filename:\n",
    "                feb_file = fp\n",
    "            elif ff == nodes_filename:\n",
    "                nodes_file = fp\n",
    "            elif ff == elems_filename:\n",
    "                elems_file = fp\n",
    "        if feb_file != None:\n",
    "            # print(feb_file)\n",
    "            # new_dm = Data_Manipulator(feb_file)\n",
    "            run_num = runs_nums[i]\n",
    "            if not SINGLE_FILE:\n",
    "                print(run_num)\n",
    "                param_val = log_df.loc[log_df[\"run#\"] == run_num][\"param\"].values[0]\n",
    "                # if type(run_num) != int() or type(run_num) != float():\n",
    "                #     run_num = log_df.loc[log_df[\"run#\"] == run_num].index.values[0]\n",
    "            else:\n",
    "                param_val = PARAM_VAL\n",
    "            if nodes_file != None and elems_file != None:\n",
    "                pickle_filename = \"data-run-{v}.pickle\".format(v = run_num)\n",
    "                new_dm.read_plot_file([nodes_file, elems_file], [nodes_keys, elems_keys], [\"node\", \"elem\"], \"\", run_num, param_val, df_dtypes)\n",
    "                m_use += mem_usage(new_dm.data)\n",
    "                f_ctn += 1\n",
    "\n",
    "                new_dm.data.to_pickle(path.join(tpm_pickles_dir, pickle_filename))\n",
    "            # dms.append(new_dm)\n",
    "\n",
    "if m_use != 0:\n",
    "    print(\"Total memory usage:\", m_use)\n",
    "    print(\"Average memory usage per df:\", m_use / f_ctn)\n",
    "\n",
    "    "
   ]
  },
  {
   "source": [
    "### Calculate nodal data\n",
    "\n",
    "Here we will be using datamanipulators dictionaries to compute nodal data based on elements information"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_nodes_data(dm, nodes_column, elems_colums, elem_data_labels, accepted_nodes=None, dtypes={}):\n",
    "    # add additional labels\n",
    "    nodes_colums.extend([\"node\",\"timestep\",\"run_ref\",\"param_val\"])\n",
    "    elems_colums.extend([\"elem\",\"timestep\",\"run_ref\",\"param_val\"])\n",
    "\n",
    "    # drop not numbers\n",
    "    nodes_df = dm.data[nodes_colums].dropna()\n",
    "    elems_df = dm.data[elems_colums].dropna()\n",
    "\n",
    "    # vectorize elems_df\n",
    "    elem_vec = elems_df.to_dict('records')\n",
    "    # set elems dict\n",
    "    elem_data_dict = dict()\n",
    "    for elem in elem_vec:\n",
    "        elem_data_dict[(elem[\"elem\"],elem[\"timestep\"],elem[\"run_ref\"])] = elem\n",
    "    \n",
    "    # vectorize nodes_df\n",
    "    nodes_vec = nodes_df.to_dict('records')\n",
    "    new_nodes_vec = []\n",
    "\n",
    "    # loop through nodes and add nodal data based on elemen value\n",
    "    for node in nodes_vec:\n",
    "\n",
    "        # get node number\n",
    "        node_num = node[\"node\"]\n",
    "        if accepted_nodes != None:\n",
    "            if node_num not in accepted_nodes:\n",
    "                continue\n",
    "        # get node refs\n",
    "        time_step = node[\"timestep\"]\n",
    "        run_ref = node[\"run_ref\"]\n",
    "\n",
    "        # get elems that are connected to given node\n",
    "        elems_c_node = dm.node_dict[int(node_num)]\n",
    "\n",
    "        # get elem_data\n",
    "        elem_data = np.zeros((1,len(elem_data_labels)))\n",
    "        for elem_num in elems_c_node:\n",
    "            elem_vals = elem_data_dict[(elem_num, time_step, run_ref)]\n",
    "            elem_data += np.array([elem_vals[v] for v in elem_data_labels])\n",
    "        elem_data = elem_data / len(elems_c_node)\n",
    "\n",
    "        # add nodal data\n",
    "        for i, el_label in enumerate(elem_data_labels):\n",
    "            node[el_label] = elem_data[0][i]\n",
    "\n",
    "        new_nodes_vec.append(node)\n",
    "\n",
    "    new_df = pd.DataFrame.from_records(new_nodes_vec)\n",
    "\n",
    "    for column_key in dtypes:\n",
    "        if column_key in new_df.columns:\n",
    "            new_df.loc[:,column_key] = new_df[column_key].astype(dtypes[column_key])\n",
    "    return new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_colums = [v for v in nodes_keys.split(\";\")]\n",
    "elems_colums = [v for v in elems_keys.split(\";\")]\n",
    "elem_data_labels = [v for v in elems_keys.split(\";\")]\n",
    "pickles_paths = search_dirs(tpm_pickles_dir, \".pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test\n",
    "df_ref = pd.read_pickle(pickles_paths[0])\n",
    "\n",
    "feb_file_ref = runs[0][0][0] # Using same file as ref since geometry is the same <-- MUST CHANGE IF WE USE DIFFERENT GEOMETRY\n",
    "dm_ref = Data_Manipulator(feb_file_ref)\n",
    "dm_ref.set_data(pickle_path=pickles_paths[0])\n",
    "endo_nodes = set(dm_ref.face_dicts[dm_ref.set_order[\"Endocardio\"]].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickles_dir = path.join(runs_dir, \"pickles\")\n",
    "endo_nodes_data_pickles_dir = path.join(pickles_dir, \"endo-nodes-data\")\n",
    "if not path.exists(endo_nodes_data_pickles_dir):\n",
    "    os.makedirs(endo_nodes_data_pickles_dir)\n",
    "\n",
    "for pp in pickles_paths:\n",
    "    # here I am getting just the endocardio node data based on ONE reference FEBIO file (same geometry)\n",
    "    dm_ref.set_data(pickle_path=pp)\n",
    "    df = calculate_nodes_data(dm_ref,nodes_colums, elems_colums, elem_data_labels, endo_nodes, dtypes=df_dtypes)\n",
    "    new_file_name = path.join(endo_nodes_data_pickles_dir, \"endo-{v}\".format(v=path.basename(pp)))\n",
    "    df.to_pickle(new_file_name)\n",
    "\n"
   ]
  },
  {
   "source": [
    "# Group dfs and calculate volumes\n",
    "\n",
    "Now we have all the datasets that we are interested in. We can now combine them and extract the volume information"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_pickles(pickles_paths, df=None, started=False, _max=2):\n",
    "\n",
    "    if len(pickles_paths) <= _max:\n",
    "        # print([path.basename(pp) for pp in pickles_paths])\n",
    "        if started:\n",
    "            df_list = [df]\n",
    "            df_list.extend([pd.read_pickle(pp) for pp in pickles_paths])\n",
    "        else:\n",
    "            df_list = [pd.read_pickle(pp) for pp in pickles_paths]\n",
    "\n",
    "        df = pd.concat(df_list, sort=False).drop_duplicates(\n",
    "        ).reset_index(drop=True)\n",
    "        \n",
    "        # print(df[\"run_ref\"].describe())\n",
    "\n",
    "        return df\n",
    "        # return pd.concat(df_list, sort=False).drop_duplicates(\n",
    "        # ).reset_index(drop=True)\n",
    "    else:\n",
    "        p = len(pickles_paths) // 2\n",
    "        fh = pickles_paths[:p]\n",
    "        sh = pickles_paths[p:]\n",
    "\n",
    "        fh_df  = combine_pickles(fh, df, True)\n",
    "        sh_df = combine_pickles(sh, fh_df, True)\n",
    "\n",
    "        return sh_df\n",
    "\n",
    "        # print(\"fh\")\n",
    "\n",
    "        # df = pd.concat([fh_df, sh_df], sort=False).drop_duplicates(\n",
    "        # ).reset_index(drop=True)\n",
    "\n",
    "        \n",
    "\n",
    "    return sh_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickles_dir = path.join(runs_dir, \"pickles\",)\n",
    "endo_nodes_data_pickles_dir = path.join(pickles_dir, \"endo-nodes-data\")\n",
    "pickles_paths = search_dirs(endo_nodes_data_pickles_dir, \".pickle\", files=[])\n",
    "\n",
    "endo_data = combine_pickles(pickles_paths)\n"
   ]
  },
  {
   "source": [
    "Once the data is combined, lets group them by param_val and timestep. This way, we will be able to iterate over them and calculate the volume by param and timestep"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endo_data_grouped = endo_data.groupby([\"param_val\", \"timestep\"])"
   ]
  },
  {
   "source": [
    "This next function is a wrapper to calculate the volume of a given nodal dataframe. It accepts the dataframe and the position references ['x','y','z']. It returns the volume in units cubic of input data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import ConvexHull\n",
    "def get_volume(df, labels=[\"x\",\"y\",\"z\"]):\n",
    "    return ConvexHull(df[labels].to_numpy(), qhull_options=\"Qt Qx Qv Q4 Q14\").volume"
   ]
  },
  {
   "source": [
    "Since we did not record the pressure data, we can define a function to return a pressure given a timestep. If we change the pressure-time curve, we can just modify this funtion."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include the equation for the pressure\n",
    "def get_linear_pressure(t):\n",
    "    if t < 0.1:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return (16/0.2) * (t - 0.1)"
   ]
  },
  {
   "source": [
    "Now, the fun part: calculate the volume!\n",
    "\n",
    "To do so, we will create a vector (one dimension list) with the computed values and transform into a dataframe afterwards. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_volume = get_volume(dm_ref.initial_pos.loc[dm_ref.initial_pos[\"node\"].isin(endo_nodes)])\n",
    "print(\"initial volume:\", initial_volume)\n",
    "\n",
    "vol_vec = list()\n",
    "for name, group in endo_data_grouped:\n",
    "    volume = get_volume(group)\n",
    "    vol_data = {\n",
    "        \"param_val\": name[0],\n",
    "        \"timestep\": name[1],\n",
    "        \"volume\": volume * 0.001,\n",
    "        \"perc_vol\": (volume - initial_volume)/(initial_volume),\n",
    "        \"pressure\": get_linear_pressure(name[1])\n",
    "    }\n",
    "    vol_vec.append(vol_data)\n",
    "vol_df = pd.DataFrame.from_records(vol_vec)"
   ]
  },
  {
   "source": [
    "# Save final dataframe"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "At last, we have our final dataframe. we will save it as a pickle and excel format."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickles_dir = path.join(runs_dir, \"pickles\",)\n",
    "final_data_dir = path.join(pickles_dir, \"final-data\")\n",
    "if not path.exists(final_data_dir):\n",
    "    os.makedirs(final_data_dir)\n",
    "\n",
    "f_filename = path.join(final_data_dir, \"final-df.pickle\")\n",
    "vol_df.to_pickle(f_filename)\n",
    "f_filename = path.join(final_data_dir, \"final-df.xlsx\")\n",
    "vol_df.to_excel(f_filename)\n"
   ]
  },
  {
   "source": [
    "## Last check the results\n",
    "\n",
    "To analyze the results, we have to group our dataset based on param_val, which will facilitate its analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_df_grouped = vol_df.groupby(\"param_val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "# grid = plt.grid(color='r', linestyle='-', linewidth=2)\n",
    "fig, ax = plt.subplots(1,1)\n",
    "legends = []\n",
    "for (name, group) in vol_df_grouped:\n",
    "    group.plot(kind='line',x='pressure',y='perc_vol',ax=ax, title=\"Pressure vs gamma\")\n",
    "    legends.append(\"gamma: {v}\".format(v=np.round(name,3)))\n",
    "    # group.plot(kind='line',x='pressure',y='perc_vol', ax=ax, secondary_y=True)\n",
    "    # group.plot(x =['pressure', 'pressure'], y=['volume', 'perc_vol'], title=name, secondary_y=True, grid=grid)\n",
    "ax.grid(color='k', linestyle='--', linewidth=0.25)\n",
    "ax.legend(legends)\n",
    "ax.set_ylabel(\"Volume %\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "# grid = plt.grid(color='r', linestyle='-', linewidth=2)\n",
    "fig2, ax2 = plt.subplots(1,1)\n",
    "legends = []\n",
    "for (name, group) in vol_df_grouped:\n",
    "    group.plot(kind='line',x='pressure',y='volume',ax=ax2, title=\"Pressure vs gamma\")\n",
    "    legends.append(\"gamma: {v}\".format(v=np.round(name,3)))\n",
    "    # group.plot(kind='line',x='pressure',y='perc_vol', ax=ax, secondary_y=True)\n",
    "    # group.plot(x =['pressure', 'pressure'], y=['volume', 'perc_vol'], title=name, secondary_y=True, grid=grid)\n",
    "ax2.grid(color='k', linestyle='--', linewidth=0.25)\n",
    "ax2.legend(legends)\n",
    "ax2.set_ylabel(\"Volume [ml]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# pp = PdfPages('Pressure-vs-gamma.pdf')\n",
    "# pp.savefig(fig1)\n",
    "# pp.savefig(fig2)\n",
    "# pp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "# grid = plt.grid(color='r', linestyle='-', linewidth=2)\n",
    "fig2, ax2 = plt.subplots(1,1)\n",
    "legends = []\n",
    "for (name, group) in vol_df_grouped:\n",
    "    group.plot(kind='line',x='timestep',y='volume',ax=ax2, title=\"Pressure vs gamma\")\n",
    "    legends.append(\"gamma: {v}\".format(v=np.round(name,3)))\n",
    "    # group.plot(kind='line',x='pressure',y='perc_vol', ax=ax, secondary_y=True)\n",
    "    # group.plot(x =['pressure', 'pressure'], y=['volume', 'perc_vol'], title=name, secondary_y=True, grid=grid)\n",
    "ax2.grid(color='k', linestyle='--', linewidth=0.25)\n",
    "ax2.legend(legends)\n",
    "ax2.set_ylabel(\"Volume [ml]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}